#+title: Homework 1: Small-scale Machine Learning with Hand-Crafted Features
#+author: Toni Kazic
#+date: <2018-03-08 Thu>



* Instructions

Please put your answers right after each question.  Answer the questions
with both links to your files (*use relative paths beginning with
../../repo/s18 in this file!*) and discuss your work: your ideas, approach,
what worked, what didn't, and what you would do differently.  The
discussion should be succinct.


Please post the completed file to canvas no later than noon on
<2018-03-12 Mon> (question 1) and <2018-03-19 Mon> (question 2).
Canvas identifies your submission, so there's no need to put your name and
student number in the file (and please don't, for FERPA reasons).



* Task

The basic plan is to take some data sets, embed them in several different
ways, and then try to distinguish bot from non-bot posts in supervised
classification. 


Each person has one embedding to produce and one ML algorithm to use on all
the embeddings.


Your description and discussion of your work goes in this file, with links
to your code, embeddings, and ML results on the repo. *Please use relative
paths beginning with ../../repo/s18 in this file!* Please label each file
with the lower-cased name of the embedding or ML algorithm.


I suggest you use [[ http://scikit-learn.org/stable/modules/linear_model.html][scikit-learn]]'s implementations of the ML algorithms, but
you are free to use any implementation provided you meet the constraints of
your problem.



** Data Sets

  [[./data/bot_scoringv2.csv][Bot Scoring V2]]

  [[./data/nonbot_scoring.csv][Non Bot Scoring]]

  [[./data/train_test_data/bot_compiled_scores.csv][Bot Compiled Scores]] 

  [[./data/train_test_data/nonbot_compiled_scores.csv][Nonbot Compiled Scores]]



** 1.  Embeddings (due <2018-03-12 Mon>)

+ Code goes on the repo at s18/new_code/embeddings.
+ Results go on the repo at s18/results/embeddings.
+ Your amended version of this file goes on canvas.


Use the wdr, dissim, and leven scores for each data set and:

   + rescale the two dimensions with the smaller dynamic ranges to the
     range of the dimension having the largest dynamic range. :samika:

   + Z-score for each dimension :rui:

   + rescale all three dimensions based on each value's rank in its range. :mark:

   + bin each dimension into 20-ciles, then recode each dimension's value
     with its bin number. :said:

   + spectral embedding, retaining the best dimensions :aquila:
       
        In the new_code directory find the file [[./new_code/embeddings/runSpectral.m][runSpectral.m]] and run this
        in order to see the spectral embeding demo.

        In the new_code directory find the file [[./new_code/embeddings/spectralEmbed.m][spectralEmbed.m]] for that
        actual spectral operation. 

     Spectral embedding in short:
       1.Compute distances between all pairs of points
       2.If the distance between the pair in focus, is below some parameter
     threshold then compute the similarity:          
            exp(-(distance)/(2*parameterVariance))
       3.Convert sum the similarities for each point, then convert to a
     diagonal matrix (called the degree)
       4.Compute the laplacian (degree-similarity matrix)
       5.Normalize the laplacian
       6.Compute the eigen vectors and values from the normalized laplacian
       7.Find the second smallest eigen lambda and store the matching eigen
     matrix as z.
       8.The output is the degree matrix^(-1/2) * z

     This is for keeping only the best dimension of the eigen vectors.

     Resuts:
      The following images show the histograms of the output data.
     [[./results/embeddings/Bot Spectral Histogram.jpg][Bot Spectral Histogram]]
     [[./results/embeddings/nonbot Spectral Histogram.jpg][Non-bot Spectral Histogram]]
     [[./results/embeddings/BotCompiled Spectral Histogram.jpg][Bot Compiled Spectral Hist]]
     [[./results/embeddings/NonBotCompiled Spectral Histogram.jpg][Non-bot Compiled Spectral Hist]]

     Also see the csv data files:
      [[./results/embeddings/bot_spectral.csv][Bot Spectral]]
      [[./results/embeddings/nonbot_spectral.csv][Non-bot Spectral]]
      [[./results/embeddings/botCompiled_spectral.csv][Bot Compiled Spectral]]
      [[./results/embeddings/nonbotCompiled_spectral.csv][Non-bot Compiled Spectral]]

     Thoughts:
      For now there are some issues with the values, but I believe this to
      be due to un optimized parameters. I am new to spectral clustering
      but I believe that the code is running correctly. 
     
:aquila: (trying to get matlab code to run inline)
#+begin_src python
x=5+1
return x
#+end_src

#+results:
: 6


#+begin_src matlab :results output
x=5
return 
disp(x)
#+end_src

#+results:





   + position of value relative to the major mode of each dimension's
     distribution :derek:

   + multi-dimensional scaling :will:




** 2.  ML algorithms (due <2018-03-19 Mon>)

+ Code goes on the repo at s18/new_code/ml_sm_set.
+ Results go on the repo at s18/results/ml_sm_set.
+ Your amended version of this file goes on canvas.



Please use the following ML algorithms for the all the different embeddings
of all the data sets.  Note each has two pairs of labelled data.


   + Logistic regression/Maximum Entropy (explore effects of tuning parameter
     values)  :mark:

   + Naive Bayes (choose algorithm based on distribution of the data) :rui:

   + Perceptron with one hidden layer, trained with back-propagation, with
     and without regularization :aquila:

   + affinity propagation :derek:

   + Gaussian process regression (Gaussian and radial basis function kernels) :samika:

   + SVM :sai:

   + spectral clustering :will:





* Grading Scale

This homework is worth 20 points. The grading scale is:  


| fraction correctly answered | points awarded |
|-----------------------------+----------------|
| >= 0.9                      |             20 |
| 0.8 -- 0.89                 |             17 |
| 0.7 -- 0.79                 |             14 |
| 0.6 -- 0.69                 |             11 |
| 0.5 -- 0.59                 |              8 |
| 0.4 -- 0.49                 |              5 |
| 0.3 -- 0.39                 |              3 |
| < 0.3                       |              0 |







* Scoring

This homework is worth 20 points, with each question worth 10 points.  The
scale is:


| question     | answer ok? |
|--------------+------------|
| 1            |            |
| 2            |            |
|--------------+------------|
| total score  |            |
| percentage   |            |
| total points |            |
#+TBLFM: @4$2=vsum(@2..@3)::@5$2=@4/20




* My Work 


Logistic regression is a statistical method for analyzing a dataset in which there are one or more independent variables that determine an outcome.

Logistic Regression Assumptions

Target variable is binary
Predictive features are interval (continuous) or categorical
Features are independent of one another
Sample size is adequate ¡V Rule of thumb: 50 records per predictor

Uses for Logistic Regression
Stock Market Predictions
Customer conversion for sales
Continuation of support based on factors

This predicts an outcome, and it also provides a probability of that prediction being correct.


#+BEGIN_SRC python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import spearmanr
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import scale
from sklearn import metrics
from sklearn import preprocessing
import seaborn as sb
import pandas as pd

from sklearn.metrics import confusion_matrix



if __name__ == '__main__':
    filename = 'Data/rescaled.csv'
    posts = pd.read_csv(filename)
    print(posts.head())

    y = posts.ix[:, 3].values
    posts_data = X = posts.ix[:, (0, 1, 2)]
    posts_data_name  = ['wdr','dissim','leven']
    print(y)

    # check how correlated the attributes.  Below zero is less than 50% correlation
    sb.heatmap(posts_data.corr())
    plt.show()
    # spearmanr_coefficient, p_value = spearmanr(wdr, leven, )
    # print('Spearmnar Rank Coorelation Coefficient %0.3f' %spearmanr_coefficient)


    print(posts.isnull().sum())
    # Checking if our target is ordinary or binary
    sb.countplot(x='is_bot', data=posts, palette='hls')
    plt.show()
    print(posts.info())

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1, random_state=47)

    LogReg = LogisticRegression()
    LogReg.fit(X_train, y_train)

    y_pred = LogReg.predict(X_test)

    confusion_matrix = confusion_matrix(y_test, y_pred)
    print('confusion_matrix')
    print(confusion_matrix)
    sb.heatmap(confusion_matrix)
    plt.show()

    return (metrics.classification_report(y_test, y_pred),confusion_matrix) 

#+END_SRC
