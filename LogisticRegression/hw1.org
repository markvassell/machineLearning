#+title: Homework 1: Small-scale Machine Learning with Hand-Crafted Features
#+author: Toni Kazic
#+date: <2018-03-08 Thu>



* Instructions

Please put your answers right after each question.  Answer the questions
with both links to your files (*use relative paths beginning with
../../repo/s18 in this file!*) and discuss your work: your ideas, approach,
what worked, what didn't, and what you would do differently.  The
discussion should be succinct.


Please post the completed file to canvas no later than noon on
<2018-03-12 Mon> (question 1) and <2018-03-19 Mon> (question 2).
Canvas identifies your submission, so there's no need to put your name and
student number in the file (and please don't, for FERPA reasons).



* Task

The basic plan is to take some data sets, embed them in several different
ways, and then try to distinguish bot from non-bot posts in supervised
classification. 


Each person has one embedding to produce and one ML algorithm to use on all
the embeddings.


Your description and discussion of your work goes in this file, with links
to your code, embeddings, and ML results on the repo. *Please use relative
paths beginning with ../../repo/s18 in this file!* Please label each file
with the lower-cased name of the embedding or ML algorithm.


I suggest you use [[ http://scikit-learn.org/stable/modules/linear_model.html][scikit-learn]]'s implementations of the ML algorithms, but
you are free to use any implementation provided you meet the constraints of
your problem.



** Data Sets

  [[./data/bot_scoringv2.csv][Bot Scoring V2]]

  [[./data/nonbot_scoring.csv][Non Bot Scoring]]

  [[./data/train_test_data/bot_compiled_scores.csv][Bot Compiled Scores]] 

  [[./data/train_test_data/nonbot_compiled_scores.csv][Nonbot Compiled Scores]]



** 1.  Embeddings (due <2018-03-12 Mon>)

+ Code goes on the repo at s18/new_code/embeddings.
+ Results go on the repo at s18/results/embeddings.
+ Your amended version of this file goes on canvas.


Use the wdr, dissim, and leven scores for each data set and:

   + rescale the two dimensions with the smaller dynamic ranges to the
     range of the dimension having the largest dynamic range. :samika:

   + Z-score for each dimension :rui:

   + rescale all three dimensions based on each value's rank in its range. :mark:

   + bin each dimension into 20-ciles, then recode each dimension's value
     with its bin number. :said:

   + spectral embedding, retaining the best dimensions :aquila:
       
        In the new_code directory find the file [[./new_code/embeddings/runSpectral.m][runSpectral.m]] and run this
        in order to see the spectral embeding demo.

        In the new_code directory find the file [[./new_code/embeddings/spectralEmbed.m][spectralEmbed.m]] for that
        actual spectral operation. 

     Spectral embedding in short:
       1.Compute distances between all pairs of points
       2.If the distance between the pair in focus, is below some parameter
     threshold then compute the similarity:          
            exp(-(distance)/(2*parameterVariance))
       3.Convert sum the similarities for each point, then convert to a
     diagonal matrix (called the degree)
       4.Compute the laplacian (degree-similarity matrix)
       5.Normalize the laplacian
       6.Compute the eigen vectors and values from the normalized laplacian
       7.Find the second smallest eigen lambda and store the matching eigen
     matrix as z.
       8.The output is the degree matrix^(-1/2) * z

     This is for keeping only the best dimension of the eigen vectors.

     Resuts:
      The following images show the histograms of the output data.
     [[./results/embeddings/Bot Spectral Histogram.jpg][Bot Spectral Histogram]]
     [[./results/embeddings/nonbot Spectral Histogram.jpg][Non-bot Spectral Histogram]]
     [[./results/embeddings/BotCompiled Spectral Histogram.jpg][Bot Compiled Spectral Hist]]
     [[./results/embeddings/NonBotCompiled Spectral Histogram.jpg][Non-bot Compiled Spectral Hist]]

     Also see the csv data files:
      [[./results/embeddings/bot_spectral.csv][Bot Spectral]]
      [[./results/embeddings/nonbot_spectral.csv][Non-bot Spectral]]
      [[./results/embeddings/botCompiled_spectral.csv][Bot Compiled Spectral]]
      [[./results/embeddings/nonbotCompiled_spectral.csv][Non-bot Compiled Spectral]]

     Thoughts:
      For now there are some issues with the values, but I believe this to
      be due to un optimized parameters. I am new to spectral clustering
      but I believe that the code is running correctly. 
     
:aquila: (trying to get matlab code to run inline)
#+begin_src python
x=5+1
return x
#+end_src

#+results:
: 6


#+begin_src matlab :results output
x=5
return 
disp(x)
#+end_src

#+results:





   + position of value relative to the major mode of each dimension's
     distribution :derek:

   + multi-dimensional scaling :will:




** 2.  ML algorithms (due <2018-03-19 Mon>)

+ Code goes on the repo at s18/new_code/ml_sm_set.
+ Results go on the repo at s18/results/ml_sm_set.
+ Your amended version of this file goes on canvas.



Please use the following ML algorithms for the all the different embeddings
of all the data sets.  Note each has two pairs of labelled data.


   + Logistic regression/Maximum Entropy (explore effects of tuning parameter
     values)  :mark:

   + Naive Bayes (choose algorithm based on distribution of the data) :rui:

   + Perceptron with one hidden layer, trained with back-propagation, with
     and without regularization :aquila:

   + affinity propagation :derek:

   + Gaussian process regression (Gaussian and radial basis function kernels) :samika:

   + SVM :sai:

   + spectral clustering :will:





* Grading Scale

This homework is worth 20 points. The grading scale is:  


| fraction correctly answered | points awarded |
|-----------------------------+----------------|
| >= 0.9                      |             20 |
| 0.8 -- 0.89                 |             17 |
| 0.7 -- 0.79                 |             14 |
| 0.6 -- 0.69                 |             11 |
| 0.5 -- 0.59                 |              8 |
| 0.4 -- 0.49                 |              5 |
| 0.3 -- 0.39                 |              3 |
| < 0.3                       |              0 |







* Scoring

This homework is worth 20 points, with each question worth 10 points.  The
scale is:


| question     | answer ok? |
|--------------+------------|
| 1            |            |
| 2            |            |
|--------------+------------|
| total score  |            |
| percentage   |            |
| total points |            |
#+TBLFM: @4$2=vsum(@2..@3)::@5$2=@4/20




* My Work 

:Mark:
Logistic regression is a statistical method for analyzing a dataset in which there are one or more independent variables
that determine an outcome.

Logistic Regression Assumptions

Target variable is binary
Predictive features are interval (continuous) or categorical
Features are independent of one another
Sample size is adequate Â¡V Rule of thumb: 50 records per predictor

Uses for Logistic Regression
Stock Market Predictions
Customer conversion for sales
Continuation of support based on factors

This predicts an outcome, and it also provides a probability of that prediction being correct.


#+BEGIN_SRC python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import spearmanr
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import scale
from sklearn import metrics
from sklearn import preprocessing
import seaborn as sb
import pandas as pd

from sklearn.metrics import confusion_matrix


def get_correlation(outFile, data):
    # check how correlated the attributes.  Below zero is less than 50% correlation
    plt.figure()
    sb.heatmap(data.corr(), annot=True, cmap="YlGnBu",vmax=1,vmin=-1,)
    plt.savefig(outFile, format='png')
    plt.close()
    # plt.show()

def binary_check(outFile, data):
    # Checking if our target is ordinary or binary
    plt.figure()
    sb.countplot(x='is_bot', data=data, palette='hls')
    plt.savefig(outFile, format='png')
    # plt.show()
    plt.close()
    print(data.info())

def generate_confusion_matrix(matrix,outFile,title):
    plt.figure()
    plt.title(title)
    plt.xlabel('Predicted')
    plt.ylabel('Actual')

    sb.heatmap(matrix, annot=True, fmt="d", cmap="YlGnBu", xticklabels=['non-bot', 'bot'],
               yticklabels=['non-bot', 'bot'])
    plt.savefig(outFile, format='png')
    # plt.show()
    plt.close()

def combined_embeddings(files):
    for name in files:
        embedding = name
        filename = 'Data/combined_embeddings/'+embedding+'.csv'
        posts = pd.read_csv(filename)
        # print(posts.head())
        if name == 'mds':
            y = posts.ix[:, 0].values
            posts_data = X = posts.ix[:, (1, 2)]
        else:
            y = posts.ix[:, 3].values
            posts_data = X = posts.ix[:, (0, 1, 2)]
        print(y)

        get_correlation('Results/'+embedding+'/combined/attribute_correlations.PNG', posts_data)

        print(posts.isnull().sum())
        binary_check('Results/' + embedding + '/combined/binary_target_check.PNG', posts)

        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1, random_state=47)

        logReg = LogisticRegression()
        logReg.fit(X_train, y_train)

        y_pred = logReg.predict(X_test)

        my_confusion_matrix = confusion_matrix(y_test, y_pred)

        print('combined '+ name + 'confusion_matrix')
        print(my_confusion_matrix)
        generate_confusion_matrix(my_confusion_matrix, 'Results/'+embedding+'/combined/confusion_matrix.PNG','combined '+ name + ' confusion matrix')


        with open('Results/'+embedding+'/combined/stats.txt', 'w+') as statsFile:
            statsFile.write(metrics.classification_report(y_test, y_pred))


def separated_embeddings(files):
    for file in files:
        if file == 'mds':
            data_range = (0,1)
        else:
            data_range = (0,1,2)

        bot = 'Data/separated_embeddings/' + file + '/bot.csv'
        bot = pd.read_csv(bot)
        bot_data = bot.ix[:, data_range]
        get_correlation('Results/' + file + '/bot/attribute_correlations.PNG', bot_data)

        nonbot = 'Data/separated_embeddings/' + file + '/nonbot.csv'
        nonbot = pd.read_csv(nonbot)
        nonbot_data = nonbot.ix[:, data_range]
        get_correlation('Results/' + file + '/nonbot/attribute_correlations.PNG', nonbot_data)


if __name__ == '__main__':
    files = ['zscore', 'recoded', 'ranked', 'rescaled', 'mds']

    combined_embeddings(files)
    separated_embeddings(files)

#+END_SRC

Here are the results for all the combined embeddings:
#+CAPTION: MDS Combined Confusion Matrix
#+NAME:   fig:MDS
[[https://github.com/markvassell/machineLearning/blob/master/LogisticRegression/Results/mds/combined/confusion_matrix.PNG][MDS]]

#+CAPTION: Ranked Combined Confusion Matrix
#+NAME:   fig:Ranked
[[https://github.com/markvassell/machineLearning/blob/master/LogisticRegression/Results/ranked/combined/confusion_matrix.PNG][Ranked]]

#+CAPTION: Recoded Combined Confusion Matrix
#+NAME:   fig:Recoded
[[https://github.com/markvassell/machineLearning/blob/master/LogisticRegression/Results/recoded/combined/confusion_matrix.PNG][Recoded]]

#+CAPTION: Rescaled Combined Confusion Matrix
#+NAME:   fig:Rescaled
[[https://github.com/markvassell/machineLearning/blob/master/LogisticRegression/Results/rescaled/combined/confusion_matrix.PNG][Rescaled]]

#+CAPTION: Spectral Combined Confusion Matrix
#+NAME:   fig:Spectral
[[https://github.com/markvassell/machineLearning/blob/master/LogisticRegression/Results/spectral/combined/confusion_matrix.PNG][Spectral]]

#+CAPTION: Zscore Combined Confusion Matrix
#+NAME:   fig:Zscore
[[https://github.com/markvassell/machineLearning/blob/master/LogisticRegression/Results/zscore/combined/confusion_matrix.PNG][Zscore]]


#+BEGIN_SRC python
import pandas as pd
# from pandas.plotting import scatter_matrix
# import matplotlib.pyplot as plt
from sklearn import model_selection
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

class style:
   PURPLE = '\033[95m'
   CYAN = '\033[96m'
   DARKCYAN = '\033[36m'
   BLUE = '\033[94m'
   GREEN = '\033[92m'
   YELLOW = '\033[93m'
   RED = '\033[91m'
   BOLD = '\033[1m'
   UNDERLINE = '\033[4m'
   END = '\033[0m'

def add_is_bot_col(is_bot, data):
    if is_bot == 0:
        data['is_bot'] = 0
    else:
        data['is_bot'] = 1

    return data

def combine_data(bots, non_bots):
    bots = add_is_bot_col(1, bots)
    non_bots = add_is_bot_col(0, non_bots)

    return bots.append(non_bots)


def prepare_data():
    all_data_set = list()
    ranked_bot_data_url = 'https://raw.githubusercontent.com/tonikazic/munlp_f17/master/s18/results/embeddings/mark/ranked_bot_scoringv2.csv?token=AI7HgZ-H4AARV4S8GS7AFB2jzf6N6dr-ks5a8SwiwA%3D%3D'
    ranked_non_bot_data_url = 'https://raw.githubusercontent.com/tonikazic/munlp_f17/master/s18/results/embeddings/mark/ranked_nonbot_scoring.csv?token=AI7HgdOvYt0Qn8utV2Mq-kKfa9fb6O4Mks5a8TBuwA%3D%3D'
    ranked_bot_data = pd.read_csv(ranked_bot_data_url)
    ranked_non_bot_data = pd.read_csv(ranked_non_bot_data_url)
    #ranked_bot_data = add_is_bot_col(1, ranked_bot_data)
    #ranked_non_bot_data = add_is_bot_col(0, ranked_non_bot_data)
    ranked_combined_data = combine_data(ranked_bot_data, ranked_non_bot_data)
    all_data_set.append(['Mark\'s Ranked',ranked_combined_data])

    rescaled_combined_data_url = 'https://raw.githubusercontent.com/tonikazic/munlp_f17/master/s18/results/embeddings/samaikya/rescaled_bot_nonbot/rescaled_bot_nonbot_Scoring.csv?token=AI7HgT70wLgEugGgHhYofm3KDo2uxqc5ks5a8dJ3wA%3D%3D'
    rescaled_combined_data = pd.read_csv(rescaled_combined_data_url)
    all_data_set.append(['Samaikya\'s Rescaled', rescaled_combined_data])

    scoring_combined_data_url = 'https://raw.githubusercontent.com/tonikazic/munlp_f17/master/s18/results/embeddings/derek/scoring_updated.csv?token=AI7HgVF1QPH42oTHwx2XiSv0_xtRs-KGks5a8dkEwA%3D%3D'
    scoring_combined_data = pd.read_csv(scoring_combined_data_url)
    all_data_set.append(['Derek\'s Scoring', scoring_combined_data])


    zscore_bot_data_url = 'https://raw.githubusercontent.com/tonikazic/munlp_f17/master/s18/results/embeddings/rui/bot_scoringv2.csv?token=AI7HgWrgkLiRu4rzBSHvO12kL0mmavynks5a8dtdwA%3D%3D'
    zscore_non_bot_data_url = 'https://raw.githubusercontent.com/tonikazic/munlp_f17/master/s18/results/embeddings/rui/nonbot_scoring.csv?token=AI7HgSnqRnMAKK5bRnAr5peOfnunsUe_ks5a8dukwA%3D%3D'
    zscore_bot_data = pd.read_csv(zscore_bot_data_url)
    zscore_non_bot_data = pd.read_csv(zscore_non_bot_data_url)
    zscore_combined_data = combine_data(zscore_bot_data, zscore_non_bot_data)
    all_data_set.append(['Rui\'s Zscore', zscore_combined_data])


    return all_data_set

def main():


    data_sets = prepare_data()

    for id, data in data_sets:
        print(style.RED + style.BOLD + style.UNDERLINE + id + ' Data:' +style.END)

        # print(scatter_matrix(combined_data))
        # plt.show()

        # Create a Validation Data set
        # split the loaded dataset into two, 80% of which we will use to train our models and 20% that we will hold back as
        # a validation data set
        validation_size = 0.15
        X = data.ix[:, (1,2,3)].values
        Y = data.ix[:, 4].values
        seed = 34
        X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size=validation_size,
                                                                                        random_state=seed)


        model_dict = {'LR' : 'Logistic Regression',
                      'LDA': 'Linear Discriminant Analysis',
                      'KNN':'K Neighbors Classifier',
                      'CART':'Decision Tree Classifier',
                      'NB':'Gaussian Naive Bayes',
                      'SVM':'Support Vector Classification'
                      }
        scoring = 'accuracy'
        # Spot Check Algorithms
        models = []
        models.append(('LR', LogisticRegression()))
        models.append(('LDA', LinearDiscriminantAnalysis()))
        models.append(('KNN', KNeighborsClassifier()))
        models.append(('CART', DecisionTreeClassifier()))
        models.append(('NB', GaussianNB()))
        models.append(('SVM', SVC()))
        # evaluate each model in turn
        results = []
        names = []
        best_model_score, best_model, model_name  = 0, None, None

        display_msg = ''
        for name, model in models:
            kfold = model_selection.KFold(n_splits=10, random_state=seed)
            cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)
            results.append(cv_results)
            names.append(name)
            display_msg += "%s  \tmean = %f \tstd = (%f) \n" % (name, cv_results.mean(), cv_results.std())

            if cv_results.mean() > best_model_score:
                best_model = model
                best_model_score = cv_results.mean()
                model_name = name


        print(display_msg)

        # Make predictions on validation dataset
        selected_model = best_model
        selected_model.fit(X_train, Y_train)
        predictions = selected_model.predict(X_validation)
        print('The best model: '+ style.GREEN + model_dict[model_name] + style.END)
        print('Accuracy: ', accuracy_score(Y_validation, predictions))
        print(confusion_matrix(Y_validation, predictions))
        print(classification_report(Y_validation, predictions))

if __name__ == '__main__':
    main()


# K Neighbors Classifier
# implements learning based on the k nearest neighbors of each query point, where k is an integer value specified by
# the user

# Linear Discriminant Analysis
# A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using
# Bayesâ rule.
# The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix.
# The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most
# discriminative directions.

# C-Support Vector Classification.
# The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which
# makes it hard to scale to dataset with more than a couple of 10000 samples.

#+END_SRC

#+results:



