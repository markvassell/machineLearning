import pandas as pd
# from pandas.plotting import scatter_matrix
# import matplotlib.pyplot as plt
from sklearn import model_selection
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

def add_is_bot_col(is_bot, data):
    if is_bot == 0:
        data['is_bot'] = 0
    else:
        data['is_bot'] = 1

    return data


def main():
    bot_data_url = 'https://raw.githubusercontent.com/tonikazic/munlp_f17/master/s18/results/embeddings/mark/ranked_bot_scoringv2.csv?token=AI7HgZ-H4AARV4S8GS7AFB2jzf6N6dr-ks5a8SwiwA%3D%3D'
    non_bot_data_url = 'https://raw.githubusercontent.com/tonikazic/munlp_f17/master/s18/results/embeddings/mark/ranked_nonbot_scoring.csv?token=AI7HgdOvYt0Qn8utV2Mq-kKfa9fb6O4Mks5a8TBuwA%3D%3D'
    bot_data = pd.read_csv(bot_data_url)
    non_bot_data = pd.read_csv(non_bot_data_url)

    bot_data = add_is_bot_col(1, bot_data)
    non_bot_data = add_is_bot_col(0, non_bot_data)

    combined_data = bot_data.append(non_bot_data)

    # print(combined_data.describe())

    # print(scatter_matrix(combined_data))
    # plt.show()

    # Create a Validation Data set
    # split the loaded dataset into two, 80% of which we will use to train our models and 20% that we will hold back as
    # a validation data set
    validation_size = 0.10
    X = combined_data.ix[:, (1,2,3)].values
    Y = combined_data.ix[:, 4].values
    seed = 34
    X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size=validation_size,
                                                                                    random_state=seed)


    model_dict = {'LR' : 'Logistic Regression',
                  'LDA': 'Linear Discriminant Analysis',
                  'KNN':'K Neighbors Classifier',
                  'CART':'Decision Tree Classifier',
                  'NB':'Gaussian Naive Bayes',
                  'SVM':'Support Vector Classification'
                  }
    scoring = 'accuracy'
    # Spot Check Algorithms
    models = []
    models.append(('LR', LogisticRegression()))
    models.append(('LDA', LinearDiscriminantAnalysis()))
    models.append(('KNN', KNeighborsClassifier()))
    models.append(('CART', DecisionTreeClassifier()))
    models.append(('NB', GaussianNB()))
    models.append(('SVM', SVC()))
    # evaluate each model in turn
    results = []
    names = []
    best_model_score, best_model, model_name  = 0, None, None
    for name, model in models:
        kfold = model_selection.KFold(n_splits=10, random_state=seed)
        cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)
        results.append(cv_results)
        names.append(name)
        msg = "%s  \tmean = %f \tstd = (%f)" % (name, cv_results.mean(), cv_results.std())
        print(msg)

        if cv_results.mean() > best_model_score:
            best_model = model
            best_model_score = cv_results.mean()
            model_name = name

    # Make predictions on validation dataset
    selected_model = best_model
    selected_model.fit(X_train, Y_train)
    predictions = selected_model.predict(X_validation)
    print('The best model: ', model_dict[model_name])
    print('Accuracy: ', accuracy_score(Y_validation, predictions))
    print(confusion_matrix(Y_validation, predictions))
    print(classification_report(Y_validation, predictions))

if __name__ == '__main__':
    main()


# K Neighbors Classifier
# implements learning based on the k nearest neighbors of each query point, where k is an integer value specified by
# the user

# Linear Discriminant Analysis
# A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using
# Bayesâ€™ rule.
# The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix.
# The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most
# discriminative directions.

# C-Support Vector Classification.
# The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which
# makes it hard to scale to dataset with more than a couple of 10000 samples.
